{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Seminar in Statistics\n\n## Personel Project","metadata":{}},{"cell_type":"code","source":"import os\nimport json\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pydicom\nfrom keras import layers\nfrom keras.applications import DenseNet121\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import Callback, ModelCheckpoint\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.initializers import Constant\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom tensorflow.python.ops import array_ops\nfrom tqdm import tqdm\n\nfrom keras import backend as K\nimport tensorflow as tf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"Using TensorFlow backend.\n","output_type":"stream"}]},{"cell_type":"code","source":"BASE_PATH = '../input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/'\nTRAIN_DIR = 'stage_2_train/'\nTEST_DIR = 'stage_2_test/'","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Load CSV","metadata":{}},{"cell_type":"markdown","source":"기존 데이터의 ID를 파일명과 type으로 구분해서 데이터프레임으로 저장","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(BASE_PATH + 'stage_2_train.csv')\nsub_df = pd.read_csv(BASE_PATH + 'stage_2_sample_submission.csv')\n\ntrain_df['filename'] = train_df['ID'].apply(lambda st: \"ID_\" + st.split('_')[1] + \".png\")\ntrain_df['type'] = train_df['ID'].apply(lambda st: st.split('_')[2])\nsub_df['filename'] = sub_df['ID'].apply(lambda st: \"ID_\" + st.split('_')[1] + \".png\")\nsub_df['type'] = sub_df['ID'].apply(lambda st: st.split('_')[2])\n\nprint(train_df.shape)\ntrain_df.head()","metadata":{"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"(4516842, 4)\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                              ID  Label          filename              type\n0          ID_12cadc6af_epidural      0  ID_12cadc6af.png          epidural\n1  ID_12cadc6af_intraparenchymal      0  ID_12cadc6af.png  intraparenchymal\n2  ID_12cadc6af_intraventricular      0  ID_12cadc6af.png  intraventricular\n3      ID_12cadc6af_subarachnoid      0  ID_12cadc6af.png      subarachnoid\n4          ID_12cadc6af_subdural      0  ID_12cadc6af.png          subdural","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Label</th>\n      <th>filename</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>ID_12cadc6af_epidural</td>\n      <td>0</td>\n      <td>ID_12cadc6af.png</td>\n      <td>epidural</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>ID_12cadc6af_intraparenchymal</td>\n      <td>0</td>\n      <td>ID_12cadc6af.png</td>\n      <td>intraparenchymal</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>ID_12cadc6af_intraventricular</td>\n      <td>0</td>\n      <td>ID_12cadc6af.png</td>\n      <td>intraventricular</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>ID_12cadc6af_subarachnoid</td>\n      <td>0</td>\n      <td>ID_12cadc6af.png</td>\n      <td>subarachnoid</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>ID_12cadc6af_subdural</td>\n      <td>0</td>\n      <td>ID_12cadc6af.png</td>\n      <td>subdural</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_df = pd.DataFrame(sub_df.filename.unique(), columns=['filename'])\nprint(test_df.shape)\ntest_df.head()","metadata":{"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"(121232, 1)\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"           filename\n0  ID_0fbf6a978.png\n1  ID_d62ec3412.png\n2  ID_cb544194b.png\n3  ID_0d62513ec.png\n4  ID_fc45b2151.png","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>ID_0fbf6a978.png</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>ID_d62ec3412.png</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>ID_cb544194b.png</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>ID_0d62513ec.png</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>ID_fc45b2151.png</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"너무 많은 개수의 데이터가 있기 때문에 150000개의 사진을 랜덤으로 선정해 'sample_df' 라는 이름으로 저장","metadata":{}},{"cell_type":"code","source":"np.random.seed(1000)\nsample_files = np.random.choice(os.listdir(BASE_PATH + TRAIN_DIR), 150000)\nsample_df = train_df[train_df.filename.apply(lambda x: x.replace('.png', '.dcm')).isin(sample_files)]","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"sample_files.shape","metadata":{"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(150000,)"},"metadata":{}}]},{"cell_type":"code","source":"print(sample_df.shape)\nsample_df.head()","metadata":{"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"(815250, 4)\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                              ID  Label          filename              type\n0          ID_12cadc6af_epidural      0  ID_12cadc6af.png          epidural\n1  ID_12cadc6af_intraparenchymal      0  ID_12cadc6af.png  intraparenchymal\n2  ID_12cadc6af_intraventricular      0  ID_12cadc6af.png  intraventricular\n3      ID_12cadc6af_subarachnoid      0  ID_12cadc6af.png      subarachnoid\n4          ID_12cadc6af_subdural      0  ID_12cadc6af.png          subdural","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Label</th>\n      <th>filename</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>ID_12cadc6af_epidural</td>\n      <td>0</td>\n      <td>ID_12cadc6af.png</td>\n      <td>epidural</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>ID_12cadc6af_intraparenchymal</td>\n      <td>0</td>\n      <td>ID_12cadc6af.png</td>\n      <td>intraparenchymal</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>ID_12cadc6af_intraventricular</td>\n      <td>0</td>\n      <td>ID_12cadc6af.png</td>\n      <td>intraventricular</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>ID_12cadc6af_subarachnoid</td>\n      <td>0</td>\n      <td>ID_12cadc6af.png</td>\n      <td>subarachnoid</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>ID_12cadc6af_subdural</td>\n      <td>0</td>\n      <td>ID_12cadc6af.png</td>\n      <td>subdural</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"하나의 사진에 여러번의 관측치가 중복되어 있기 때문에, 각 사진별 type을 저장","metadata":{}},{"cell_type":"code","source":"pivot_df = sample_df[['Label', 'filename', 'type']].drop_duplicates().pivot(\n    index='filename', columns='type', values='Label').reset_index()\nprint(pivot_df.shape)\npivot_df.head()","metadata":{"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"(135873, 7)\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"type          filename  any  epidural  intraparenchymal  intraventricular  \\\n0     ID_0000950d7.png    0         0                 0                 0   \n1     ID_0002081b6.png    1         0                 1                 0   \n2     ID_000259ccf.png    0         0                 0                 0   \n3     ID_00027c277.png    0         0                 0                 0   \n4     ID_00027cbb1.png    0         0                 0                 0   \n\ntype  subarachnoid  subdural  \n0                0         0  \n1                0         0  \n2                0         0  \n3                0         0  \n4                0         0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>type</th>\n      <th>filename</th>\n      <th>any</th>\n      <th>epidural</th>\n      <th>intraparenchymal</th>\n      <th>intraventricular</th>\n      <th>subarachnoid</th>\n      <th>subdural</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>ID_0000950d7.png</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>ID_0002081b6.png</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>ID_000259ccf.png</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>ID_00027c277.png</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>ID_00027cbb1.png</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Rescale, Resize and Convert to PNG","metadata":{}},{"cell_type":"markdown","source":"dcm 확장자로 만들어진 파일의 크기를 조절한 후 png로 다시 저장","metadata":{}},{"cell_type":"code","source":"def window_image(img, window_center, window_width, intercept, slope, rescale=True):\n\n    img = (img*slope +intercept)\n    img_min = window_center - window_width//2\n    img_max = window_center + window_width//2\n    img[img<img_min] = img_min\n    img[img>img_max] = img_max\n    \n    if rescale:\n        # Extra rescaling to 0-1, not in the original notebook\n        img = (img - img_min) / (img_max - img_min)\n    \n    return img\n    \ndef get_first_of_dicom_field_as_int(x):\n\n    if type(x) == pydicom.multival.MultiValue:\n        return int(x[0])\n    else:\n        return int(x)\n\ndef get_windowing(data):\n    dicom_fields = [data[('0028','1050')].value, #window center\n                    data[('0028','1051')].value, #window width\n                    data[('0028','1052')].value, #intercept\n                    data[('0028','1053')].value] #slope\n    return [get_first_of_dicom_field_as_int(x) for x in dicom_fields]","metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def save_and_resize(filenames, load_dir):    \n    save_dir = '/kaggle/tmp/'\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    for filename in tqdm(filenames):\n        path = load_dir + filename\n        new_path = save_dir + filename.replace('.dcm', '.png')\n        \n        dcm = pydicom.dcmread(path)\n        window_center , window_width, intercept, slope = get_windowing(dcm)\n        img = dcm.pixel_array\n        img = window_image(img, window_center, window_width, intercept, slope)\n        \n        resized = cv2.resize(img, (224, 224))\n        res = cv2.imwrite(new_path, resized)","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"save_and_resize(filenames=sample_files, load_dir=BASE_PATH + TRAIN_DIR)\nsave_and_resize(filenames=os.listdir(BASE_PATH + TEST_DIR), load_dir=BASE_PATH + TEST_DIR)","metadata":{"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"100%|██████████| 150000/150000 [53:24<00:00, 46.80it/s] \n100%|██████████| 121232/121232 [44:58<00:00, 44.93it/s] \n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Data Generator\n\n이미지 데이터의 경우에 사진을 조금씩 움직여 눈에는 비슷하게 보이지만, 실제 데이터는 완전히 다른 데이터를 생성하는 방법을 많이 사용한다.\n\nKeras의 ImageDataGenerator와 flow_from_data 메소드를 사용해 검증용 데이터를 만드는 함수를 정의한다.","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 64\n\ndef create_datagen():\n    return ImageDataGenerator(validation_split=0.15)\n\ndef create_test_gen():\n    return ImageDataGenerator().flow_from_dataframe(\n        test_df,\n        directory='/kaggle/tmp/',\n        x_col='filename',\n        class_mode=None,\n        target_size=(224, 224),\n        batch_size=BATCH_SIZE,\n        shuffle=False\n    )\n\ndef create_flow(datagen, subset):\n    return datagen.flow_from_dataframe(\n        pivot_df, \n        directory='/kaggle/tmp/',\n        x_col='filename', \n        y_col=['any', 'epidural', 'intraparenchymal', \n               'intraventricular', 'subarachnoid', 'subdural'],\n        class_mode='raw',\n        target_size=(224, 224),\n        batch_size=BATCH_SIZE,\n        subset=subset\n    )","metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"data_generator = create_datagen()\ntrain_gen = create_flow(data_generator, 'training')\nval_gen = create_flow(data_generator, 'validation')\ntest_gen = create_test_gen()","metadata":{"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Found 115493 validated image filenames.\nFound 20380 validated image filenames.\nFound 121232 validated image filenames.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"markdown","source":"### Loss function\n\n불균형한 데이터에서의 성능을 높이기 위해 고안된 Focal Loss Function을 사용한다.","metadata":{}},{"cell_type":"code","source":"def focal_loss(prediction_tensor, target_tensor, weights=None, alpha=0.25, gamma=2):\n\n    sigmoid_p = tf.nn.sigmoid(prediction_tensor)\n    zeros = array_ops.zeros_like(sigmoid_p, dtype=sigmoid_p.dtype)\n    \n    # For poitive prediction, only need consider front part loss, back part is 0;\n    # target_tensor > zeros <=> z=1, so poitive coefficient = z - p.\n    pos_p_sub = array_ops.where(target_tensor > zeros, target_tensor - sigmoid_p, zeros)\n    \n    # For negative prediction, only need consider back part loss, front part is 0;\n    # target_tensor > zeros <=> z=1, so negative coefficient = 0.\n    neg_p_sub = array_ops.where(target_tensor > zeros, zeros, sigmoid_p)\n    per_entry_cross_ent = - alpha * (pos_p_sub ** gamma) * tf.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0)) \\\n                          - (1 - alpha) * (neg_p_sub ** gamma) * tf.log(tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))\n    return tf.reduce_sum(per_entry_cross_ent)","metadata":{"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### DenseNet\n\nDenseNet은 레이어가 많아졌을 때 이전 레이어의 출력값과 거리가 멀어졌을 때 그 정보가 희박해지는 단점을 보완한 딥러닝 기법이다. \n\n기존 ResNet은 바로 이전의 출력값만을 사용했지만, DenseNet은 해당 레이어 이전에 나왔던 모든 출력값을 사용한다는 차이점이 있다.\n\n합성함수는 Batch Normalization -> ReLU -> 3x3 Conv Layer 로 이어지는 함수이다.\n\n사전학습된 가중치를 적용한 DenseNet 레이어를 사용한다.","metadata":{}},{"cell_type":"code","source":"densenet = DenseNet121(\n    weights='../input/densenet-keras/DenseNet-BC-121-32-no-top.h5',\n    include_top=False,\n    input_shape=(224,224,3)\n)","metadata":{"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def build_model():\n    model = Sequential()\n    model.add(densenet)\n    model.add(layers.GlobalAveragePooling2D())\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(6, activation='sigmoid', bias_initializer=Constant(value=-5.5)))\n    \n    model.compile(\n        loss=focal_loss,\n        optimizer=Adam(lr=0.001),\n        metrics=['accuracy']\n    )\n    \n    return model","metadata":{"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"model = build_model()\nmodel.summary()","metadata":{"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndensenet121 (Model)          (None, 7, 7, 1024)        7037504   \n_________________________________________________________________\nglobal_average_pooling2d_1 ( (None, 1024)              0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 1024)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 6)                 6150      \n=================================================================\nTotal params: 7,043,654\nTrainable params: 6,960,006\nNon-trainable params: 83,648\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"checkpoint = ModelCheckpoint(\n    'model.h5', \n    monitor='val_loss', \n    verbose=0, \n    save_best_only=True, \n    save_weights_only=False,\n    mode='auto'\n)\n\ntotal_steps = sample_files.shape[0] / BATCH_SIZE\n\nhistory = model.fit_generator(\n    train_gen,\n    steps_per_epoch=2000,\n    validation_data=val_gen,\n    validation_steps=total_steps * 0.15,\n    callbacks=[checkpoint],\n    epochs=5\n)","metadata":{"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Epoch 1/5\n2000/2000 [==============================] - 1077s 539ms/step - loss: 0.4124 - accuracy: 0.4455 - val_loss: 0.8615 - val_accuracy: 0.3205\nEpoch 2/5\n2000/2000 [==============================] - 1036s 518ms/step - loss: 0.2142 - accuracy: 0.5783 - val_loss: 0.0271 - val_accuracy: 0.1366\nEpoch 3/5\n2000/2000 [==============================] - 1038s 519ms/step - loss: 0.2031 - accuracy: 0.5696 - val_loss: 0.2619 - val_accuracy: 0.5934\nEpoch 4/5\n2000/2000 [==============================] - 1040s 520ms/step - loss: 0.1884 - accuracy: 0.5564 - val_loss: 0.0662 - val_accuracy: 0.9906\nEpoch 5/5\n2000/2000 [==============================] - 1039s 520ms/step - loss: 0.1779 - accuracy: 0.5405 - val_loss: 0.6064 - val_accuracy: 1.0000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"model.load_weights('model.h5')\ny_test = model.predict_generator(\n    test_gen,\n    steps=len(test_gen),\n    verbose=1\n)","metadata":{"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"1895/1895 [==============================] - 295s 156ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# Append the output predicts in the wide format to the y_test\ntest_df = test_df.join(pd.DataFrame(y_test, columns=[\n    'any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural'\n]))\n\n# Unpivot table, i.e. wide (N x 6) to long format (6N x 1)\ntest_df = test_df.melt(id_vars=['filename'])\n\n# Combine the filename column with the variable column\ntest_df['ID'] = test_df.filename.apply(lambda x: x.replace('.png', '')) + '_' + test_df.variable\ntest_df['Label'] = test_df['value']\n\ntest_df[['ID', 'Label']].to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}